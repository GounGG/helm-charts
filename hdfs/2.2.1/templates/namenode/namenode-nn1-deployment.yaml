apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ template "hdfs.fullname" . }}-namenode-nn1
  labels:
    {{- include "hdfs.labels" . | nindent 4 }}
    app.kubernetes.io/component: namenode
    namenode-id: nn1
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      {{- include "hdfs.selectorLabels" . | nindent 6 }}
      app.kubernetes.io/component: namenode
      namenode-id: nn1
  template:
    metadata:
      annotations:
        checksum/config: {{ include (print $.Template.BasePath "/config.yaml") . | sha256sum }}
        checksum/secrets: {{ include (print $.Template.BasePath "/secrets.yaml") . | sha256sum }}
      labels:
        {{- include "hdfs.labels" . | nindent 8 }}
        app.kubernetes.io/component: namenode
        namenode-id: nn1
    spec:
      hostname: {{ template "hdfs.fullname" . }}-namenode-nn1
      terminationGracePeriodSeconds: 180
      {{- if and .Values.config.rackAwareness .Values.config.rackAwareness.nodeTopologyLabel }}
      serviceAccountName: {{ template "hdfs.namenodeServiceAccountName" . }}
      {{- end }}
      initContainers:
      - name: wait-for-journalnode-startup
        image: {{ .Values.alpine.repository}}:{{ .Values.alpine.tag }}
        imagePullPolicy: {{ .Values.alpine.imagePullPolicy }}
        command: [
          "/bin/sh",
          "-c",
          'for i in $(seq 1 300); do nc -z -w3 {{ template "hdfs.fullname" . }}-journalnode-0.{{ template "hdfs.fullname" . }}-journalnodes:{{ .Values.journalnode.ports.journalnode }} && exit 0 || sleep 1; done; exit 1'
        ]
      - name: wait-for-zookeeper
        image: {{ .Values.alpine.repository}}:{{ .Values.alpine.tag }}
        imagePullPolicy: {{ .Values.alpine.imagePullPolicy }}
        command: [
          "/bin/sh",
          "-c",
          'echo "Waiting for ZooKeeper..."; 
           for i in $(seq 1 60); do 
             nc -z -w3 {{ template "hdfs.fullname" . }}-zk-0.{{ template "hdfs.fullname" . }}-zk-svc 2181 && 
             nc -z -w3 {{ template "hdfs.fullname" . }}-zk-1.{{ template "hdfs.fullname" . }}-zk-svc 2181 && 
             nc -z -w3 {{ template "hdfs.fullname" . }}-zk-2.{{ template "hdfs.fullname" . }}-zk-svc 2181 && 
             echo "ZooKeeper is ready" && sleep 60 && exit 0; 
             echo "Waiting... ($i/60)";
             sleep 2; 
           done; 
           echo "Timeout waiting for ZooKeeper"; 
           exit 1'
        ]
      - name: format-data
        image: {{ .Values.namenode.repository }}:{{ .Values.namenode.tag }}
        imagePullPolicy: {{ .Values.namenode.imagePullPolicy }}
        command: ["/bin/bash","-c"]
        args:
        - |
            export HADOOP_CONF_DIR={{ .Values.config.path }}
            export HDFS_NAMENODE_OPTS="-Dhdfs.audit.logger=INFO,NullAppender -Ddfs.ha.namenode.id=nn1"
            # Guard: format only when VERSION does not exist
            if [ -f /data/dfs/name/current/VERSION ]; then
              echo "VERSION exists, skip format"
              exit 0
            fi
            echo "No VERSION found, formatting NN (nn1)..."
            hdfs namenode -format -nonInteractive -D dfs.ha.namenode.id=nn1
        env:
        - name: HADOOP_CONF_DIR
          value: {{ .Values.config.path }}
        volumeMounts:
        - name: log
          mountPath: /var/log/hadoop
        - name: config
          mountPath: {{ .Values.config.path }}
          readOnly: true
        - name: secrets
          mountPath: {{ .Values.secrets.path }}
          readOnly: true
        - name: data
          mountPath: /data
      - name: format-zkfc
        image: {{ .Values.namenode.repository }}:{{ .Values.namenode.tag }}
        imagePullPolicy: {{ .Values.namenode.imagePullPolicy }}
        command: ["/bin/bash", "-c"]
        args:
        - |
          export HADOOP_CONF_DIR={{ .Values.config.path }}
          echo "Checking if ZooKeeper is already formatted for HDFS HA..."
          # Check if ZK is already formatted; do not force re-format
          if hdfs zkfc -formatZK -nonInteractive 2>&1 | grep -q "already exists"; then
            echo "ZooKeeper already formatted, skipping"
            exit 0
          fi
          echo "Formatting ZooKeeper for HDFS HA (first-time only)..."
          hdfs zkfc -formatZK -nonInteractive
          echo "ZooKeeper formatted successfully"
        env:
        - name: HADOOP_CONF_DIR
          value: {{ .Values.config.path }}
        volumeMounts:
        - name: config
          mountPath: {{ .Values.config.path }}
          readOnly: true
        - name: secrets
          mountPath: {{ .Values.secrets.path }}
          readOnly: true
        - name: log
          mountPath: /var/log/hadoop
        - name: data
          mountPath: /data
      containers:
      - name: zkfc
        image: {{ .Values.namenode.repository }}:{{ .Values.namenode.tag }}
        imagePullPolicy: {{ .Values.namenode.imagePullPolicy }}
        ports:
        - name: zkfc-rpc
          containerPort: {{ .Values.namenode.ports.zkfcRpc }}
          protocol: TCP
        command: ["/bin/bash", "-c"]
        args:
        - |
          export HADOOP_CONF_DIR={{ .Values.config.path }}
          export HDFS_ZKFC_OPTS="-Ddfs.ha.namenode.id=nn1"
          
          # Wait for NameNode to be ready
          echo "Waiting for NameNode to start..."
          for i in $(seq 1 300); do
            if curl -s -f http://localhost:{{ .Values.namenode.ports.http }}/jmx > /dev/null 2>&1; then
              echo "NameNode HTTP is responsive"
              sleep 5
              break
            fi
            echo "Waiting for NameNode... (attempt $i/300)"
            sleep 2
          done
          
          echo "Starting ZKFC with ID: nn1"
          exec hdfs zkfc -D dfs.ha.namenode.id=nn1
        env:
        - name: HADOOP_CONF_DIR
          value: {{ .Values.config.path }}
        {{- range $key, $value := .Values.namenode.extraEnvVars }}
        - name: {{ $key | upper | replace "." "_" }}
          value: {{ $value | quote }}
        {{- end }}
        volumeMounts:
        - name: config
          mountPath: {{ .Values.config.path }}
          readOnly: true
        - name: secrets
          mountPath: {{ .Values.secrets.path }}
          readOnly: true
        - name: fence-script
          mountPath: /scripts/fence-namenode.sh
          subPath: fence-namenode.sh
          readOnly: true
        - name: log
          mountPath: /var/log/hadoop
        - name: data
          mountPath: /data
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"
      - name: namenode
        image: {{ .Values.namenode.repository }}:{{ .Values.namenode.tag }}
        imagePullPolicy: {{ .Values.namenode.imagePullPolicy }}
        command: ["/bin/bash", "-c"]
        args:
        - |
          export HADOOP_CONF_DIR={{ .Values.config.path }}
          export HDFS_NAMENODE_OPTS="-Dhdfs.audit.logger=INFO,NullAppender -Ddfs.ha.namenode.id=nn1"
          
          echo "Starting NameNode nn1..."
          exec hdfs namenode -D dfs.ha.namenode.id=nn1
        env:
        - name: HADOOP_CONF_DIR
          value: {{ .Values.config.path }}
        {{- range $key, $value := .Values.namenode.extraEnvVars }}
        - name: {{ $key | upper | replace "." "_" }}
          value: {{ $value | quote }}
        {{- end }}
        volumeMounts:
        - name: config
          mountPath: {{ .Values.config.path }}
          readOnly: true
        - name: secrets
          mountPath: {{ .Values.secrets.path }}
          readOnly: true
        {{- if and .Values.config.rackAwareness .Values.config.rackAwareness.nodeTopologyLabel }}
        - name: rack-awareness
          mountPath: /scripts/resolve-rack.sh
          subPath: resolve-rack.sh
        {{- end }}
        - name: data
          mountPath: /data
        - name: log
          mountPath: /var/log/hadoop
        ports:
        - name: http
          containerPort: {{ .Values.namenode.ports.http }}
        - name: https
          containerPort: {{ .Values.namenode.ports.https }}
        - name: client-rpc
          containerPort: {{ .Values.namenode.ports.clientRpc }}
        - name: service-rpc
          containerPort: {{ .Values.namenode.ports.serviceRpc }}
        - name: lifeline-rpc
          containerPort: {{ .Values.namenode.ports.lifelineRpc }}
        livenessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - hdfs dfsadmin -fs hdfs://localhost -safemode get
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 30
          successThreshold: 1
          failureThreshold: 2
        resources:
          {{- toYaml .Values.namenode.resources | nindent 10 }}
      volumes:
      - name: log
        emptyDir: {}
      - name: config
        configMap:
          name: {{ template "hdfs.fullname" . }}
          optional: false
      - name: secrets
        secret:
          secretName: {{ template "hdfs.fullname" . }}
          optional: false
      {{- if .Values.config.rackAwareness }}
      {{- if .Values.config.rackAwareness.nodeTopologyLabel }}
      - name: rack-awareness
        configMap:
          name: {{ template "hdfs.fullname" . }}-rack-awareness
          defaultMode: 0755
      {{- end }}
      {{- end }}
      - name: fence-script
        configMap:
          name: {{ template "hdfs.fullname" . }}-fence-script
          defaultMode: 0755
      - name: data
        persistentVolumeClaim:
          claimName: {{ template "hdfs.fullname" $ }}-namenode-nn1-data
      {{- with .Values.namenode.securityContext }}
      securityContext:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      affinity:
      {{- if .Values.namenode.affinity }}
        {{- toYaml .Values.namenode.affinity | nindent 8 }}
      {{- end }}
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                - namenode
            topologyKey: kubernetes.io/hostname
      {{- with .Values.namenode.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.namenode.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.namenode.imagePullSecrets }}
      imagePullSecrets:
      {{- range . }}
      - name: {{ . }}
      {{- end }}
      {{- end }}

